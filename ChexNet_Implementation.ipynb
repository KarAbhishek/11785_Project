{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-785 Project\n",
    "### ChexNet\n",
    "\n",
    "We followed the training strategy described in the official paper, and a ten crop method is adopted both in validation and test. Compared with the original CheXNet, the per-class AUROC of our reproduced model is almost the same. We have also proposed a slightly-improved model which achieves a mean AUROC of 0.847 (v.s. 0.841 of the original CheXNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim    #optim.lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision #torchvision.datasets, torchvision.models, torchvision.transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau #Reduce learning rate when a metric has stopped improving\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, os, copy\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "# 'PIL' is the Python Imaging Library, \n",
    "# 'Image' module provides a class to represent a PIL image. \n",
    "## it provides factory functions, like load images from files, and to create new images.\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#Used to compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw(path, name):\n",
    "    file = Path(path) / name\n",
    "    if file.exists():\n",
    "        return np.load(file,encoding='bytes')\n",
    "    else:\n",
    "        raise Exception(\"File not found chutia!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(numpy_array):\n",
    "    # Numpy array -> Tensor\n",
    "    return torch.from_numpy(numpy_array)\n",
    "\n",
    "\n",
    "def to_variable(tensor):\n",
    "    # Tensor -> Variable (on GPU if possible)\n",
    "    if torch.cuda.is_available():\n",
    "        # Tensor -> GPU Tensor\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from github of arnoweng/CheXNet\n",
    "def compute_AUCs(y_hat, y_true, args):\n",
    "    AUROCs = []\n",
    "    \n",
    "    y_hat = normalize(y_hat, axis=1, norm='l1')\n",
    "    print (y_true.shape)\n",
    "    print (y_hat.shape)\n",
    "    #print (roc_auc_score(y_true, y_hat))\n",
    "    for i in range(args.n_classes):\n",
    "        AUROCs.append(roc_auc_score(y_true[:, i], y_hat[:, i]))\n",
    "        \n",
    "    AUROC_avg = np.array(AUROCs).mean()\n",
    "    print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(args.n_classes):\n",
    "        print('The AUROC of {} is {}'.format(args.disease_categories[i], AUROCs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    #!!!def __init__(self, classCount, isTrained):\n",
    "    def __init__(self, args):\n",
    "        #!!!super(DenseNet121, self).__init__()\n",
    "        super().__init__()\n",
    "        self.densenet121 = None\n",
    "        \n",
    "        if args.backprop_pretained:\n",
    "            #Fixed Feature Extractor\n",
    "            #freeze the weights for all of the network except that of the final fully connected layer. \n",
    "            \n",
    "            self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "            for param in self.densenet121.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            #parameters = filter(lambda p: p.requires_grad, self.densenet121.parameters())\n",
    "            #for param in parameters:\n",
    "            #    param.requires_grad = False\n",
    "                \n",
    "        else:\n",
    "            #Finetuning \n",
    "            #initialize the network with a pretrained networt. Rest of the training looks as usual.\n",
    "            self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "            \n",
    "        \n",
    "        # Parameters of newly constructed modules have requires_grad=True by default\n",
    "        #fc -> contains the last layer of network (only for resnet)\n",
    "        #classifier -> -> contains the last layer of network (only for densenet)\n",
    "        \n",
    "        ##in RESNET last layer is from 2048 to 1000\n",
    "        #num_features = dcnn.fc.in_features \n",
    "        \n",
    "        ##in DENSENET last layer is from 1024 to 1000\n",
    "        num_features = self.densenet121.classifier.in_features\n",
    "        \n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, args.n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        #for x in self.densenet121.classifier.parameters():\n",
    "        #    print (x)\n",
    "        \n",
    "    #each image should be size 224x224 as original paper of Densenet states\n",
    "    def forward(self, input_val):\n",
    "        y = self.densenet121(input_val)\n",
    "        return y\n",
    "    \n",
    "                    \n",
    "    def initialize_weigths(self, args):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset\n",
    "\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using \n",
    "`normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class ChestXray(torch.utils.data.Dataset):\n",
    "class ChestXray (torch.utils.data.TensorDataset):\n",
    "    def __init__(self, args, dataset_list_file, path, is_val=False):\n",
    "        \n",
    "        \n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                         [0.229, 0.224, 0.225])\n",
    "        \n",
    "        #used in variation of CheXnet of Weng,Zhuang,Tian\n",
    "        self.transform1 = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.TenCrop(224), #return  list of 10 images \n",
    "                                transforms.Lambda (lambda crops: torch.stack([transforms.ToTensor()(x) for x in crops])),\n",
    "                                transforms.Lambda(lambda crops: torch.stack([normalize(x) for x in crops]))\n",
    "                            ])\n",
    "        \n",
    "        #used in original paper of CheXnet of Rajpurkar,Irvin,Zhu,Ng\n",
    "        self.transform2 = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize\n",
    "                            ])\n",
    "        self.is_val = is_val\n",
    "            \n",
    "        image_names = []\n",
    "        labels = []\n",
    "        tmp = \"all_images\"\n",
    "        with open(os.path.join(path, dataset_list_file), \"r\") as file:\n",
    "            for line in file:\n",
    "                items = line.split(',')\n",
    "                label = [int(i) for i in items[1:]]\n",
    "                labels.append(label)\n",
    "\n",
    "                image_filename = items[0]\n",
    "                image_name = os.path.join(path,tmp, image_filename)\n",
    "                image_names.append(image_name)\n",
    "                \n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        print (\"There are {} images in the Images Dataset\".format(len(self.image_names)))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        image_filename = self.image_names[index]\n",
    "        image = Image.open(image_filename).convert('RGB')\n",
    "        label = torch.FloatTensor(self.labels[index])\n",
    "        if args.transform == \"transform_1\":\n",
    "            if self.is_val == False:\n",
    "                image = self.transform1(image)\n",
    "            else:\n",
    "                image = self.transform2(image)\n",
    "        elif args.transform == \"transform_2\":\n",
    "            image = self.transform2(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#routine to compute loss based on Validation dataset\n",
    "def validate_routine(model, args, val_datalist,path):\n",
    "    \n",
    "    model.eval() #DO NOT FORGET to do evaluation\n",
    "    \n",
    "    loss = nn.BCELoss()\n",
    "    \n",
    "    dataset = ChestXray(args, val_datalist, path, is_val=True)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                    dataset, batch_size=64, shuffle=False,\n",
    "                    num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "    \n",
    "    losses = []\n",
    "    for i,(input_val,labels) in enumerate(data_loader): \n",
    "            \n",
    "        #forward pass\n",
    "        #print (\"val batch processing: \",i)\n",
    "        prediction = model(to_variable(input_val))\n",
    "\n",
    "        #print(\"Finished forward pass\")\n",
    "        val_loss = loss(prediction, to_variable(labels))\n",
    "        losses.append(val_loss.data.cpu().numpy())\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_routine(args, path, train_datalist, val_datalist):\n",
    "    \n",
    "    #open files and load content\n",
    "    \n",
    "    # Create the network\n",
    "    model = DenseNet121(args)  \n",
    "    \n",
    "    #Initialize weitgths\n",
    "    #my_model.initialize_weigths()\n",
    "    \n",
    "    \n",
    "    #Choose the loss function / optimizer\n",
    "    loss = nn.BCELoss(size_average = True)\n",
    "    \n",
    "    #choose optimizer\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=args.learn_rate,\n",
    "                             weight_decay=args.l2)\n",
    "    scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 5, mode = 'min')\n",
    "                         \n",
    "    print (\"Created Neural Network arquitecture\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Move the network and the optimizer to the GPU\n",
    "        print (\"Moving to GPU\")\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        loss = loss.cuda()\n",
    "    \n",
    "    dataset = ChestXray(args, train_datalist, path, is_val=False)\n",
    "    \n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                    dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                    num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "    \n",
    "    print (\"Created data objects\")\n",
    "    \n",
    "    losses= []\n",
    "    for epoch in range(args.epochs): \n",
    "        model.train()\n",
    "        t0 = datetime.datetime.now()\n",
    "        losses = []\n",
    "        for i,(input_val,labels) in enumerate(data_loader): \n",
    "\n",
    "            #if transform_1 using, we got a vector of 5 DIM\n",
    "            #and we only need a 4 DIM\n",
    "            if args.transform == 'transform_1':\n",
    "                \n",
    "                bs, n_crops, c, h, w = input_val.size() #e.g(4,10,3,224,244)\n",
    "                input_val = input_val.view(-1, c, h, w) #e.g(40,3,224,224)\n",
    "                bs, n_classes = labels.size() #e.g(4,14)\n",
    "                labels = labels.unsqueeze(2).repeat(1, n_crops, 1 )\n",
    "                labels = labels.contiguous().view(bs*n_crops, n_classes)\n",
    "                \n",
    "            prediction = model(to_variable(input_val))\n",
    "\n",
    "            #print(\"Finished forward pass\")\n",
    "            #print (prediction.shape)\n",
    "            \n",
    "            train_loss = loss(prediction, to_variable(labels))\n",
    "            optim.zero_grad()# Reset the gradients NEVER FORGET THIS\n",
    "            train_loss.backward()\n",
    "            optim.step() # Update the network\n",
    "            \n",
    "            losses.append(train_loss.data.cpu().numpy())\n",
    "\n",
    "            \n",
    "            if i % 200 == 0:\n",
    "                print('Minibatch ',i,train_loss.data.cpu().numpy())\n",
    "            if i % 800 == 0:\n",
    "                val_loss = validate_routine(model, args, val_datalist, path)\n",
    "                scheduler.step(np.asscalar(np.mean(val_loss)))\n",
    "                model.train()\n",
    "            \n",
    "            \n",
    "        print ('EPOCH', end='\\t')\n",
    "        print(\"Epoch {} Train Loss: {:.4f}\".format(epoch, np.asscalar(np.mean(losses))), end='\\t')\n",
    "        print(\"Epoch {} Validation Loss: {:.4f}\".format(epoch, np.asscalar(np.mean(val_loss))), end='\\t')\n",
    "        print (\"Epoch Time: {}\".format(datetime.datetime.now()-t0))\n",
    "        \n",
    "        torch.save(model.state_dict(), 'ChexNet_model3.pytorch')\n",
    "        \n",
    "    return model,losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters and dataset and RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_categories = {'Atelectasis': 0, 'Cardiomegaly': 1, 'Effusion': 2,\n",
    "                          'Infiltration': 3, 'Mass': 4, 'Nodule': 5, 'Pneumonia': 6,\n",
    "                          'Pneumothorax': 7, 'Consolidation': 8, 'Edema': 9,\n",
    "                          'Emphysema': 10, 'Fibrosis': 11, 'Pleural_Thickening': 12, 'Hernia': 13,\n",
    "                          }\n",
    "\n",
    "disease_categories2 = {val:key for (key, val) in disease_categories.items()}\n",
    "\n",
    "col_nanes = ['FileName', 'Atelectasis', 'Cardiomegaly', 'Effusion', \n",
    "             'Infiltration', 'Mass', 'Nodule', 'Pneumonia', \n",
    "             'Pneumothorax', 'Consolidation', 'Edema', \n",
    "             'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #in GPU\n",
    "\n",
    "args = namedtuple('args', ['batch_size','save_directory',\n",
    "                           'load_file','backprop_pretained',\n",
    "                           'num_workers','pin_memory',\n",
    "                           'epochs','n_classes','transform',\n",
    "                           'learn_rate', 'l2',\n",
    "                           'disease_categories'])(\n",
    "                            64, 'output/trained_model', \n",
    "                            'transfer_model/model.pth.tar', True,\n",
    "                            16,True,\n",
    "                            15, 14,'transform_1',\n",
    "                            0.0001, 0.1e-08,\n",
    "                            disease_categories2)\n",
    "\n",
    "'''\n",
    "#in CPU\n",
    "args = namedtuple('args', ['batch_size','save_directory',\n",
    "                           'load_file','backprop_pretained',\n",
    "                           'num_workers','pin_memory',\n",
    "                           'epochs','n_classes','transform',\n",
    "                           'learn_rate', 'l2',\n",
    "                           'disease_categories'])(\n",
    "                            32, 'output/trained_model', \n",
    "                            'transfer_model/model.pth.tar', True,\n",
    "                            0,False,\n",
    "                            5, 14,'transform_2',\n",
    "                            0.00001, 0.1e-08,\n",
    "                            disease_categories2)\n",
    "'''\n",
    "\n",
    "path = \"dataset\"\n",
    "train = \"train_set.csv\" \n",
    "val = \"val_set.csv\"\n",
    "test = \"test_set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-25 19:06:33.286761\n",
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Neural Network arquitecture\n",
      "Moving to GPU\n",
      "There are 78826 images in the Images Dataset\n",
      "Created data objects\n",
      "Minibatch  0 0.6986119747161865\n",
      "There are 11140 images in the Images Dataset\n",
      "Minibatch  200 0.21243299543857574\n",
      "Minibatch  400 0.19965021312236786\n",
      "Minibatch  600 0.15897907316684723\n",
      "Minibatch  800 0.20092640817165375\n",
      "There are 11140 images in the Images Dataset\n",
      "Minibatch  1000 0.17773345112800598\n",
      "Minibatch  1200 0.164979487657547\n",
      "EPOCH\tEpoch 0 Train Loss: 0.1946\tEpoch 0 Validation Loss: 0.1803\tEpoch Time: 1:42:15.649643\n",
      "Minibatch  0 0.17030403017997742\n",
      "There are 11140 images in the Images Dataset\n",
      "Minibatch  200 0.1878003627061844\n",
      "Minibatch  400 0.18348920345306396\n",
      "Minibatch  600 0.19703911244869232\n",
      "Minibatch  800 0.18137864768505096\n",
      "There are 11140 images in the Images Dataset\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now()\n",
    "print (a)\n",
    "print (\"Start Training\")\n",
    "model,losses = training_routine(args, path, train, val)\n",
    "print (\"I am done training\")\n",
    "b = datetime.datetime.now()\n",
    "print (b)\n",
    "print (b-a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weng = False\n",
    "if use_weng:\n",
    "    load_model = torch.load('transfer_model/model.pth.tar', map_location=lambda storage, loc: storage)\n",
    "\n",
    "    model = DenseNet121(args)\n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in load_model.items():\n",
    "        if k == 'state_dict':\n",
    "            for k2,v in v.items():\n",
    "                #print (k2)\n",
    "                name = k2[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "    # load params\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print (\"Model Loaded, using already trained model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to GPU\n",
      "There are 22152 images in the Images Dataset\n",
      "val batch processing:  1\n",
      "val batch processing:  2\n",
      "val batch processing:  3\n",
      "val batch processing:  4\n",
      "val batch processing:  5\n",
      "val batch processing:  6\n",
      "val batch processing:  7\n",
      "val batch processing:  8\n",
      "val batch processing:  9\n",
      "val batch processing:  10\n",
      "val batch processing:  11\n",
      "val batch processing:  12\n",
      "val batch processing:  13\n",
      "val batch processing:  14\n",
      "val batch processing:  15\n",
      "val batch processing:  16\n",
      "val batch processing:  17\n",
      "val batch processing:  18\n",
      "val batch processing:  19\n",
      "val batch processing:  21\n",
      "val batch processing:  22\n",
      "val batch processing:  23\n",
      "val batch processing:  24\n",
      "val batch processing:  25\n",
      "val batch processing:  26\n",
      "val batch processing:  27\n",
      "val batch processing:  28\n",
      "val batch processing:  29\n",
      "val batch processing:  30\n",
      "val batch processing:  31\n",
      "val batch processing:  32\n",
      "val batch processing:  33\n",
      "val batch processing:  34\n",
      "val batch processing:  35\n",
      "val batch processing:  36\n",
      "val batch processing:  37\n",
      "val batch processing:  38\n",
      "val batch processing:  39\n",
      "val batch processing:  41\n",
      "val batch processing:  42\n",
      "val batch processing:  43\n",
      "val batch processing:  44\n",
      "val batch processing:  45\n",
      "val batch processing:  46\n",
      "val batch processing:  47\n",
      "val batch processing:  48\n",
      "val batch processing:  49\n",
      "val batch processing:  50\n",
      "val batch processing:  51\n",
      "val batch processing:  52\n",
      "val batch processing:  53\n",
      "val batch processing:  54\n",
      "val batch processing:  55\n",
      "val batch processing:  56\n",
      "val batch processing:  57\n",
      "val batch processing:  58\n",
      "val batch processing:  59\n",
      "val batch processing:  61\n",
      "val batch processing:  62\n",
      "val batch processing:  63\n",
      "val batch processing:  64\n",
      "val batch processing:  65\n",
      "val batch processing:  66\n",
      "val batch processing:  67\n",
      "val batch processing:  68\n",
      "val batch processing:  69\n",
      "val batch processing:  70\n",
      "val batch processing:  71\n",
      "val batch processing:  72\n",
      "val batch processing:  73\n",
      "val batch processing:  74\n",
      "val batch processing:  75\n",
      "val batch processing:  76\n",
      "val batch processing:  77\n",
      "val batch processing:  78\n",
      "val batch processing:  79\n",
      "val batch processing:  81\n",
      "val batch processing:  82\n",
      "val batch processing:  83\n",
      "val batch processing:  84\n",
      "val batch processing:  85\n",
      "val batch processing:  86\n",
      "(22152, 14)\n",
      "(22152, 14)\n",
      "The average AUROC is 0.679\n",
      "The AUROC of Atelectasis is 0.6406096981917049\n",
      "The AUROC of Cardiomegaly is 0.6800817382395566\n",
      "The AUROC of Effusion is 0.7288424773864505\n",
      "The AUROC of Infiltration is 0.5384996004973043\n",
      "The AUROC of Mass is 0.5753880650294986\n",
      "The AUROC of Nodule is 0.6035420785256662\n",
      "The AUROC of Pneumonia is 0.5957072608616284\n",
      "The AUROC of Pneumothorax is 0.7298815534886969\n",
      "The AUROC of Consolidation is 0.7119627809094822\n",
      "The AUROC of Edema is 0.822888148544922\n",
      "The AUROC of Emphysema is 0.7044018976469857\n",
      "The AUROC of Fibrosis is 0.7071243037968423\n",
      "The AUROC of Pleural_Thickening is 0.6461906776431365\n",
      "The AUROC of Hernia is 0.8141235894819112\n",
      "Predict Time: 0:04:36.940620\n"
     ]
    }
   ],
   "source": [
    "use_from_disk = True\n",
    "if use_from_disk:\n",
    "    load_model = torch.load('ChexNet_model2.pytorch', map_location=lambda storage, loc: storage)\n",
    "    #load_model = torch.load('ChexNet_model2.pytorch')\n",
    "    model = DenseNet121(args)\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in load_model.items():\n",
    "        #print (k)\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    # load params\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    #model.load_state_dict(load_model)\n",
    "    if torch.cuda.is_available():\n",
    "        # Move the network and the optimizer to the GPU\n",
    "        print (\"Moving to GPU\")\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    #print(model) \n",
    "\n",
    "\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "model.eval() #DO NOT FORGET to do evaluation\n",
    "\n",
    "\n",
    "dataset = ChestXray(args, test, path)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=256, shuffle=False,\n",
    "                num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "\n",
    "\n",
    "y_hat = []\n",
    "y_true = []\n",
    "for i,(input_val,labels) in enumerate(data_loader): \n",
    "\n",
    "    #forward pass\n",
    "    if i % 20 == 0:\n",
    "        print (\"val batch processing: \",i)\n",
    "    \n",
    "    if args.transform == 'transform_1':\n",
    "\n",
    "            bs, n_crops, c, h, w = input_val.size() #e.g(4,10,3,224,244)\n",
    "            input_val = input_val.view(-1, c, h, w) #e.g(40,3,224,224)\n",
    "            bs, n_classes = labels.size() #e.g(4,14)\n",
    "            labels = labels.unsqueeze(2).repeat(1, n_crops, 1 )\n",
    "            labels = labels.contiguous().view(bs*n_crops, n_classes)\n",
    "                \n",
    "    prediction = model(to_variable(input_val))\n",
    "    #print (\"Donde Forward Pass\")\n",
    "    y_true.append(labels.cpu().numpy().astype(int))\n",
    "    y_hat.append(prediction.data.cpu().numpy())\n",
    "    \n",
    "\n",
    "y_hat = np.vstack(y_hat)\n",
    "y_true = np.vstack(y_true)\n",
    "compute_AUCs(y_hat, y_true, args)\n",
    "print (\"Predict Time: {}\".format(datetime.datetime.now()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 14)\n",
      "(165, 14)\n",
      "The average AUROC is 0.859\n",
      "The AUROC of Atelectasis is 0.9205465587044535\n",
      "The AUROC of Cardiomegaly is 0.9374262101534828\n",
      "The AUROC of Effusion is 0.9317375886524822\n",
      "The AUROC of Infiltration is 0.49387630128597676\n",
      "The AUROC of Mass is 0.9601226993865031\n",
      "The AUROC of Nodule is 0.8991769547325104\n",
      "The AUROC of Pneumonia is 0.6707317073170731\n",
      "The AUROC of Pneumothorax is 0.9375\n",
      "The AUROC of Consolidation is 0.73375\n",
      "The AUROC of Edema is 0.9878048780487805\n",
      "The AUROC of Emphysema is 0.9503105590062111\n",
      "The AUROC of Fibrosis is 0.7670807453416149\n",
      "The AUROC of Pleural_Thickening is 0.834355828220859\n",
      "The AUROC of Hernia is 1.0\n",
      "Predict Time: 0:06:48.713071\n"
     ]
    }
   ],
   "source": [
    "compute_AUCs(prediction.data, labels, args)\n",
    "print (\"Predict Time: {}\".format(datetime.datetime.now()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHINESE MODEL TO BEAT\n",
    "\n",
    "* The average AUROC is 0.859\n",
    "* The AUROC of Atelectasis is 0.9205465587044535\n",
    "* The AUROC of Cardiomegaly is 0.9374262101534828\n",
    "* The AUROC of Effusion is 0.9317375886524822\n",
    "* The AUROC of Infiltration is 0.49387630128597676\n",
    "* The AUROC of Mass is 0.9601226993865031\n",
    "* The AUROC of Nodule is 0.8991769547325104\n",
    "* The AUROC of Pneumonia is 0.6707317073170731\n",
    "* The AUROC of Pneumothorax is 0.9375\n",
    "* The AUROC of Consolidation is 0.73375\n",
    "* The AUROC of Edema is 0.9878048780487805\n",
    "* The AUROC of Emphysema is 0.9503105590062111\n",
    "* The AUROC of Fibrosis is 0.7670807453416149\n",
    "* The AUROC of Pleural_Thickening is 0.834355828220859\n",
    "* The AUROC of Hernia is 1.0\n",
    "\n",
    "#### Model without training\n",
    "The average AUROC is 0.439\n",
    "* The AUROC of Atelectasis is 0.4600202429149797\n",
    "* The AUROC of Cardiomegaly is 0.49350649350649356\n",
    "* The AUROC of Effusion is 0.5774231678486997\n",
    "* The AUROC of Infiltration is 0.5272504592774034\n",
    "* The AUROC of Mass is 0.3098159509202454\n",
    "* The AUROC of Nodule is 0.3786008230452675\n",
    "* The AUROC of Pneumonia is 0.426829268292683\n",
    "* The AUROC of Pneumothorax is 0.23750000000000004\n",
    "* The AUROC of Consolidation is 0.5125000000000001\n",
    "* The AUROC of Edema is 0.5914634146341464\n",
    "* The AUROC of Emphysema is 0.3944099378881988\n",
    "* The AUROC of Fibrosis is 0.14440993788819875\n",
    "* The AUROC of Pleural_Thickening is 0.7668711656441718\n",
    "* The AUROC of Hernia is 0.3292682926829268\n",
    "\n",
    "#### Model trained 3 epochs, transform1\n",
    "* The average AUROC is 0.522\n",
    "* The AUROC of Atelectasis is 0.5600628216848379\n",
    "* The AUROC of Cardiomegaly is 0.4994946999599024\n",
    "* The AUROC of Effusion is 0.5750399977244025\n",
    "* The AUROC of Infiltration is 0.5460339649011673\n",
    "* The AUROC of Mass is 0.5018732586055604\n",
    "* The AUROC of Nodule is 0.5095718525039765\n",
    "* The AUROC of Pneumonia is 0.4791604866435438\n",
    "* The AUROC of Pneumothorax is 0.5447996763720493\n",
    "* The AUROC of Consolidation is 0.5780586014503319\n",
    "* The AUROC of Edema is 0.5659363209503817\n",
    "* The AUROC of Emphysema is 0.44840684512882306\n",
    "* The AUROC of Fibrosis is 0.5362195673894978\n",
    "* The AUROC of Pleural_Thickening is 0.5302204565319748\n",
    "* The AUROC of Hernia is 0.43561190809913514\n",
    "Predict Time: 0:03:35.336959\n",
    "\n",
    "#### Model Trained 15 epochs, transform1\n",
    "\n",
    "* The average AUROC is 0.626\n",
    "* The AUROC of Atelectasis is 0.6099339225725543\n",
    "* The AUROC of Cardiomegaly is 0.6010881987356465\n",
    "* The AUROC of Effusion is 0.7138617014446589\n",
    "* The AUROC of Infiltration is 0.5488878033845505\n",
    "* The AUROC of Mass is 0.5329107421959458\n",
    "* The AUROC of Nodule is 0.5597421920386231\n",
    "* The AUROC of Pneumonia is 0.5141474040099794\n",
    "* The AUROC of Pneumothorax is 0.68892335847419\n",
    "* The AUROC of Consolidation is 0.6874902348764493\n",
    "* The AUROC of Edema is 0.789283092744088\n",
    "* The AUROC of Emphysema is 0.6213053225614307\n",
    "* The AUROC of Fibrosis is 0.6504512954572436\n",
    "* The AUROC of Pleural_Thickening is 0.5792289384426821\n",
    "* The AUROC of Hernia is 0.663563257739683\n",
    "\n",
    "### Model Trained with 15 epochs and scheduler, transform1\n",
    "* The average AUROC is 0.679\n",
    "* The AUROC of Atelectasis is 0.6406096981917049\n",
    "* The AUROC of Cardiomegaly is 0.6800817382395566\n",
    "* The AUROC of Effusion is 0.7288424773864505\n",
    "* The AUROC of Infiltration is 0.5384996004973043\n",
    "* The AUROC of Mass is 0.5753880650294986\n",
    "* The AUROC of Nodule is 0.6035420785256662\n",
    "* The AUROC of Pneumonia is 0.5957072608616284\n",
    "* The AUROC of Pneumothorax is 0.7298815534886969\n",
    "* The AUROC of Consolidation is 0.7119627809094822\n",
    "* The AUROC of Edema is 0.822888148544922\n",
    "* The AUROC of Emphysema is 0.7044018976469857\n",
    "* The AUROC of Fibrosis is 0.7071243037968423\n",
    "* The AUROC of Pleural_Thickening is 0.6461906776431365\n",
    "* The AUROC of Hernia is 0.8141235894819112\n",
    "\n",
    "\n",
    "### Model Trained with 15 epochs and scheduler, transform2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
