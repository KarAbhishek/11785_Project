{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-785 Project\n",
    "### ChexNet\n",
    "\n",
    "We followed the training strategy described in the official paper, and a ten crop method is adopted both in validation and test. Compared with the original CheXNet, the per-class AUROC of our reproduced model is almost the same. We have also proposed a slightly-improved model which achieves a mean AUROC of 0.847 (v.s. 0.841 of the original CheXNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim    #optim.lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision #torchvision.datasets, torchvision.models, torchvision.transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau #Reduce learning rate when a metric has stopped improving\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, os, copy\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "# 'PIL' is the Python Imaging Library, \n",
    "# 'Image' module provides a class to represent a PIL image. \n",
    "## it provides factory functions, like load images from files, and to create new images.\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#Used to compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw(path, name):\n",
    "    file = Path(path) / name\n",
    "    if file.exists():\n",
    "        return np.load(file,encoding='bytes')\n",
    "    else:\n",
    "        raise Exception(\"File not found chutia!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(numpy_array):\n",
    "    # Numpy array -> Tensor\n",
    "    return torch.from_numpy(numpy_array)\n",
    "\n",
    "\n",
    "def to_variable(tensor):\n",
    "    # Tensor -> Variable (on GPU if possible)\n",
    "    if torch.cuda.is_available():\n",
    "        # Tensor -> GPU Tensor\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from github of arnoweng/CheXNet\n",
    "def compute_AUCs(y_hat, y_true, args):\n",
    "    AUROCs = []\n",
    "    \n",
    "    y_hat = normalize(y_hat, axis=1, norm='l1')\n",
    "    print (y_true.shape)\n",
    "    print (y_hat.shape)\n",
    "    #print (roc_auc_score(y_true, y_hat))\n",
    "    for i in range(args.n_classes):\n",
    "        AUROCs.append(roc_auc_score(y_true[:, i], y_hat[:, i]))\n",
    "        \n",
    "    AUROC_avg = np.array(AUROCs).mean()\n",
    "    print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(args.n_classes):\n",
    "        print('The AUROC of {} is {}'.format(args.disease_categories[i], AUROCs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    #!!!def __init__(self, classCount, isTrained):\n",
    "    def __init__(self, args):\n",
    "        #!!!super(DenseNet121, self).__init__()\n",
    "        super().__init__()\n",
    "        self.densenet121 = None\n",
    "        \n",
    "        if args.backprop_pretained:\n",
    "            #Fixed Feature Extractor\n",
    "            #freeze the weights for all of the network except that of the final fully connected layer. \n",
    "            \n",
    "            self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "            for param in self.densenet121.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            #parameters = filter(lambda p: p.requires_grad, self.densenet121.parameters())\n",
    "            #for param in parameters:\n",
    "            #    param.requires_grad = False\n",
    "                \n",
    "        else:\n",
    "            #Finetuning \n",
    "            #initialize the network with a pretrained networt. Rest of the training looks as usual.\n",
    "            self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "            \n",
    "        \n",
    "        # Parameters of newly constructed modules have requires_grad=True by default\n",
    "        #fc -> contains the last layer of network (only for resnet)\n",
    "        #classifier -> -> contains the last layer of network (only for densenet)\n",
    "        \n",
    "        ##in RESNET last layer is from 2048 to 1000\n",
    "        #num_features = dcnn.fc.in_features \n",
    "        \n",
    "        ##in DENSENET last layer is from 1024 to 1000\n",
    "        num_features = self.densenet121.classifier.in_features\n",
    "        \n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, args.n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    #each image should be size 224x224 as original paper of Densenet states\n",
    "    def forward(self, input_val):\n",
    "        y = self.densenet121(input_val)\n",
    "        return y\n",
    "    \n",
    "                    \n",
    "    def initialize_weigths(self, args):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset\n",
    "\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using \n",
    "`normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class ChestXray(torch.utils.data.Dataset):\n",
    "class ChestXray (torch.utils.data.TensorDataset):\n",
    "    def __init__(self, args, dataset_list_file, path, is_val=False):\n",
    "        \n",
    "        \n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                         [0.229, 0.224, 0.225])\n",
    "        \n",
    "        #used in variation of CheXnet of Weng,Zhuang,Tian\n",
    "        self.transform1 = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.TenCrop(224), #return  list of 10 images \n",
    "                                transforms.Lambda (lambda crops: torch.stack([transforms.ToTensor()(x) for x in crops])),\n",
    "                                transforms.Lambda(lambda crops: torch.stack([normalize(x) for x in crops]))\n",
    "                            ])\n",
    "        \n",
    "        #used in original paper of CheXnet of Rajpurkar,Irvin,Zhu,Ng\n",
    "        self.transform2 = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize\n",
    "                            ])\n",
    "        self.is_val = is_val\n",
    "            \n",
    "        image_names = []\n",
    "        labels = []\n",
    "        tmp = \"all_images\"\n",
    "        with open(os.path.join(path, dataset_list_file), \"r\") as file:\n",
    "            for line in file:\n",
    "                items = line.split(',')\n",
    "                label = [int(i) for i in items[1:]]\n",
    "                labels.append(label)\n",
    "\n",
    "                image_filename = items[0]\n",
    "                image_name = os.path.join(path,tmp, image_filename)\n",
    "                image_names.append(image_name)\n",
    "                \n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        print (\"There are {} images in the Images Dataset\".format(len(self.image_names)))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        image_filename = self.image_names[index]\n",
    "        image = Image.open(image_filename).convert('RGB')\n",
    "        label = torch.FloatTensor(self.labels[index])\n",
    "        if args.transform == \"transform_1\":\n",
    "            if self.is_val == False:\n",
    "                image = self.transform1(image)\n",
    "            else:\n",
    "                image = self.transform2(image)\n",
    "        elif args.transform == \"transform_2\":\n",
    "            image = self.transform2(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#routine to compute loss based on Validation dataset\n",
    "def validate_routine(model, args, val_datalist,path):\n",
    "    \n",
    "    model.eval() #DO NOT FORGET to do evaluation\n",
    "    \n",
    "    loss = nn.BCELoss()\n",
    "    \n",
    "    dataset = ChestXray(args, val_datalist, path, is_val=True)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                    dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                    num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "    \n",
    "    losses = []\n",
    "    for i,(input_val,labels) in enumerate(data_loader): \n",
    "            \n",
    "        #forward pass\n",
    "        #print (\"val batch processing: \",i)\n",
    "        prediction = model(to_variable(input_val))\n",
    "\n",
    "        #print(\"Finished forward pass\")\n",
    "        val_loss = loss(prediction, to_variable(labels))\n",
    "        losses.append(val_loss.data.cpu().numpy())\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_routine(args, path, train_datalist, val_datalist):\n",
    "    \n",
    "    #open files and load content\n",
    "    \n",
    "    # Create the network\n",
    "    if args.load_disk_model==True:\n",
    "        load_model = torch.load(args.load_checkpoint_filename, map_location=lambda storage, loc: storage)\n",
    "        #load_model = torch.load('ChexNet_model2.pytorch')\n",
    "        model = DenseNet121(args)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in load_model.items():\n",
    "            #print (k)\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "        # load params\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    elif args.load_dis_model == False:\n",
    "        model = DenseNet121(args)  \n",
    "    \n",
    "    #Initialize weitgths\n",
    "    #my_model.initialize_weigths()\n",
    "    \n",
    "    \n",
    "    #Choose the loss function / optimizer\n",
    "    loss = nn.BCELoss(size_average = True)\n",
    "    \n",
    "    #choose optimizer\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=args.learn_rate,\n",
    "                             weight_decay=args.l2)\n",
    "    scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 10, mode = 'min')\n",
    "                         \n",
    "    print (\"Created Neural Network arquitecture\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Move the network and the optimizer to the GPU\n",
    "        print (\"Moving to GPU\")\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        loss = loss.cuda()\n",
    "    \n",
    "    dataset = ChestXray(args, train_datalist, path, is_val=False)\n",
    "    \n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                    dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                    num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "    \n",
    "    print (\"Created data objects\")\n",
    "    \n",
    "    losses= []\n",
    "    for epoch in range(args.epochs): \n",
    "        model.train()\n",
    "        t0 = datetime.datetime.now()\n",
    "        losses = []\n",
    "        for i,(input_val,labels) in enumerate(data_loader): \n",
    "\n",
    "            #if transform_1 using, we got a vector of 5 DIM\n",
    "            #and we only need a 4 DIM\n",
    "            if args.transform == 'transform_1':  \n",
    "                bs, n_crops, c, h, w = input_val.size() #e.g(4,10,3,224,244)\n",
    "                input_val = input_val.view(-1, c, h, w) #e.g(40,3,224,224)\n",
    "                bs, n_classes = labels.size() #e.g(4,14)\n",
    "                labels = labels.unsqueeze(2).repeat(1, n_crops, 1 )\n",
    "                labels = labels.contiguous().view(bs*n_crops, n_classes)\n",
    "                \n",
    "            prediction = model(to_variable(input_val))\n",
    "\n",
    "            #print(\"Finished forward pass\")\n",
    "            #print (prediction.shape)\n",
    "            \n",
    "            train_loss = loss(prediction, to_variable(labels))\n",
    "            optim.zero_grad()# Reset the gradients NEVER FORGET THIS\n",
    "            train_loss.backward()\n",
    "            optim.step() # Update the network\n",
    "            \n",
    "            losses.append(train_loss.data.cpu().numpy())\n",
    "\n",
    "            \n",
    "            if i % 200 == 0:\n",
    "                print('Minibatch ',i,train_loss.data.cpu().numpy())\n",
    "            if i % 800 == 0:\n",
    "                val_loss = validate_routine(model, args, val_datalist, path)\n",
    "                scheduler.step(np.asscalar(np.mean(val_loss)))\n",
    "                model.train()\n",
    "            \n",
    "            \n",
    "        print ('EPOCH', end='\\t')\n",
    "        print (\"Epoch {} Train Loss: {:.4f}\".format(epoch, np.asscalar(np.mean(losses))), end='\\t')\n",
    "        print (\"Epoch {} Validation Loss: {:.4f}\".format(epoch, np.asscalar(np.mean(val_loss))), end='\\t')\n",
    "        print (\"Epoch Time: {}\".format(datetime.datetime.now()-t0))\n",
    "        \n",
    "        torch.save(model.state_dict(), args.save_checkpoint_filename)\n",
    "        \n",
    "    return model,losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters and dataset and RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_categories = {'Atelectasis': 0, 'Cardiomegaly': 1, 'Effusion': 2,\n",
    "                          'Infiltration': 3, 'Mass': 4, 'Nodule': 5, 'Pneumonia': 6,\n",
    "                          'Pneumothorax': 7, 'Consolidation': 8, 'Edema': 9,\n",
    "                          'Emphysema': 10, 'Fibrosis': 11, 'Pleural_Thickening': 12, 'Hernia': 13,\n",
    "                          }\n",
    "\n",
    "disease_categories2 = {val:key for (key, val) in disease_categories.items()}\n",
    "\n",
    "col_nanes = ['FileName', 'Atelectasis', 'Cardiomegaly', 'Effusion', \n",
    "             'Infiltration', 'Mass', 'Nodule', 'Pneumonia', \n",
    "             'Pneumothorax', 'Consolidation', 'Edema', \n",
    "             'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in GPU\n",
    "args = {'load_disk_model':True, 'load_checkpoint_filename':'ChexNet_model4.pytorch',\n",
    "        'save_checkpoint_filename':'ChexNet_model4.pytorch',\n",
    "        'backprop_pretained':True, #if False do finetuning if True just Fixed Feature Extractor\n",
    "        'n_classes':14, 'transform':'transform_2', 'disease_categories':disease_categories2,\n",
    "        'num_workers':16, 'pin_memory':True, #used for DataLoader\n",
    "        'batch_size':16, 'epochs':8,\n",
    "        'learn_rate':0.000001, 'l2':0.1e-08, #used in optimization \n",
    "       }\n",
    "\n",
    "#in CPU\n",
    "#args['num_workers'] = 16\n",
    "#args['pin_memory'] = True\n",
    "\n",
    "args = namedtuple('args', args.keys())(**args)\n",
    "\n",
    "path = \"dataset\"\n",
    "train = \"train_set.csv\" \n",
    "val = \"val_set.csv\"\n",
    "test = \"test_set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-03 16:40:51.038897\n",
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Neural Network arquitecture\n",
      "Moving to GPU\n",
      "There are 78826 images in the Images Dataset\n",
      "Created data objects\n",
      "Minibatch  0 0.20403613150119781\n",
      "There are 11140 images in the Images Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-377:\n",
      "Process Process-389:\n",
      "Process Process-370:\n",
      "Process Process-390:\n",
      "Process Process-388:\n",
      "Process Process-386:\n",
      "Process Process-391:\n",
      "Process Process-382:\n",
      "Process Process-371:\n",
      "Process Process-387:\n",
      "Process Process-369:\n",
      "Process Process-383:\n",
      "Process Process-372:\n",
      "Process Process-376:\n",
      "Process Process-384:\n",
      "Process Process-381:\n",
      "Process Process-378:\n",
      "Process Process-373:\n",
      "Traceback (most recent call last):\n",
      "Process Process-380:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-385:\n",
      "Process Process-375:\n",
      "Process Process-374:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-379:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 55, in __getitem__\n",
      "    image = self.transform2(image)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 55, in __getitem__\n",
      "    image = self.transform2(image)\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 55, in __getitem__\n",
      "    image = self.transform2(image)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 55, in __getitem__\n",
      "    image = self.transform2(image)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 55, in __getitem__\n",
      "    image = self.transform2(image)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 546, in __call__\n",
      "    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 55, in __getitem__\n",
      "    image = self.transform2(image)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 331, in resized_crop\n",
      "    img = resize(img, size, interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"<ipython-input-23-4f22b2c75cfd>\", line 47, in __getitem__\n",
      "    image = Image.open(image_filename).convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 546, in __call__\n",
      "    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 206, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 546, in __call__\n",
      "    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 546, in __call__\n",
      "    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 331, in resized_crop\n",
      "    img = resize(img, size, interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 70, in to_tensor\n",
      "    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 81, in to_tensor\n",
      "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 331, in resized_crop\n",
      "    img = resize(img, size, interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 331, in resized_crop\n",
      "    img = resize(img, size, interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 206, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 206, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 206, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4594e5448de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Start Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"I am done training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-b1344d6c486a>\u001b[0m in \u001b[0;36mtraining_routine\u001b[0;34m(args, path, train_datalist, val_datalist)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m800\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_datalist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9c2a0a089dde>\u001b[0m in \u001b[0;36mvalidate_routine\u001b[0;34m(model, args, val_datalist, path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# ensure that the worker exits on process exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now()\n",
    "print (a)\n",
    "print (\"Start Training\")\n",
    "model,losses = training_routine(args, path, train, val)\n",
    "print (\"I am done training\")\n",
    "b = datetime.datetime.now()\n",
    "print (b)\n",
    "print (b-a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_weng = False\n",
    "if use_weng:\n",
    "    load_model = torch.load('transfer_model/model.pth.tar', map_location=lambda storage, loc: storage)\n",
    "\n",
    "    model = DenseNet121(args)\n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in load_model.items():\n",
    "        if k == 'state_dict':\n",
    "            for k2,v in v.items():\n",
    "                #print (k2)\n",
    "                name = k2[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "    # load params\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print (\"Model Loaded, using already trained model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to GPU\n",
      "There are 22152 images in the Images Dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b216ea6c80b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChestXray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m data_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 num_workers=args.num_workers, pin_memory=args.pin_memory)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ags' is not defined"
     ]
    }
   ],
   "source": [
    "use_from_disk = True\n",
    "if use_from_disk:\n",
    "    load_model = torch.load(args.load_checkpoint_filename, map_location=lambda storage, loc: storage)\n",
    "    #load_model = torch.load('ChexNet_model2.pytorch')\n",
    "    model = DenseNet121(args)\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in load_model.items():\n",
    "        #print (k)\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    # load params\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    #model.load_state_dict(load_model)\n",
    "    if torch.cuda.is_available():\n",
    "        # Move the network and the optimizer to the GPU\n",
    "        print (\"Moving to GPU\")\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    #print(model) \n",
    "\n",
    "\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "model.eval() #DO NOT FORGET to do evaluation\n",
    "\n",
    "\n",
    "dataset = ChestXray(args, test, path, is_val=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=ags.batch_size, shuffle=False,\n",
    "                num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "\n",
    "\n",
    "y_hat = []\n",
    "y_true = []\n",
    "for i,(input_val,labels) in enumerate(data_loader): \n",
    "\n",
    "    #forward pass\n",
    "    if i % 50 == 0:\n",
    "        print (\"val batch processing: \",i)\n",
    "                \n",
    "    prediction = model(to_variable(input_val))\n",
    "    #print (\"Donde Forward Pass\")\n",
    "    y_true.append(labels.cpu().numpy().astype(int))\n",
    "    y_hat.append(prediction.data.cpu().numpy())\n",
    "    \n",
    "\n",
    "y_hat = np.vstack(y_hat)\n",
    "y_true = np.vstack(y_true)\n",
    "compute_AUCs(y_hat, y_true, args)\n",
    "print (\"Predict Time: {}\".format(datetime.datetime.now()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHINESE MODEL TO BEAT\n",
    "\n",
    "* The average AUROC is 0.859\n",
    "* The AUROC of Atelectasis is 0.9205465587044535\n",
    "* The AUROC of Cardiomegaly is 0.9374262101534828\n",
    "* The AUROC of Effusion is 0.9317375886524822\n",
    "* The AUROC of Infiltration is 0.49387630128597676\n",
    "* The AUROC of Mass is 0.9601226993865031\n",
    "* The AUROC of Nodule is 0.8991769547325104\n",
    "* The AUROC of Pneumonia is 0.6707317073170731\n",
    "* The AUROC of Pneumothorax is 0.9375\n",
    "* The AUROC of Consolidation is 0.73375\n",
    "* The AUROC of Edema is 0.9878048780487805\n",
    "* The AUROC of Emphysema is 0.9503105590062111\n",
    "* The AUROC of Fibrosis is 0.7670807453416149\n",
    "* The AUROC of Pleural_Thickening is 0.834355828220859\n",
    "* The AUROC of Hernia is 1.0\n",
    "\n",
    "#### Model without training\n",
    "The average AUROC is 0.439\n",
    "* The AUROC of Atelectasis is 0.4600202429149797\n",
    "* The AUROC of Cardiomegaly is 0.49350649350649356\n",
    "* The AUROC of Effusion is 0.5774231678486997\n",
    "* The AUROC of Infiltration is 0.5272504592774034\n",
    "* The AUROC of Mass is 0.3098159509202454\n",
    "* The AUROC of Nodule is 0.3786008230452675\n",
    "* The AUROC of Pneumonia is 0.426829268292683\n",
    "* The AUROC of Pneumothorax is 0.23750000000000004\n",
    "* The AUROC of Consolidation is 0.5125000000000001\n",
    "* The AUROC of Edema is 0.5914634146341464\n",
    "* The AUROC of Emphysema is 0.3944099378881988\n",
    "* The AUROC of Fibrosis is 0.14440993788819875\n",
    "* The AUROC of Pleural_Thickening is 0.7668711656441718\n",
    "* The AUROC of Hernia is 0.3292682926829268\n",
    "\n",
    "#### Model trained 3 epochs, transform1\n",
    "* The average AUROC is 0.522\n",
    "* The AUROC of Atelectasis is 0.5600628216848379\n",
    "* The AUROC of Cardiomegaly is 0.4994946999599024\n",
    "* The AUROC of Effusion is 0.5750399977244025\n",
    "* The AUROC of Infiltration is 0.5460339649011673\n",
    "* The AUROC of Mass is 0.5018732586055604\n",
    "* The AUROC of Nodule is 0.5095718525039765\n",
    "* The AUROC of Pneumonia is 0.4791604866435438\n",
    "* The AUROC of Pneumothorax is 0.5447996763720493\n",
    "* The AUROC of Consolidation is 0.5780586014503319\n",
    "* The AUROC of Edema is 0.5659363209503817\n",
    "* The AUROC of Emphysema is 0.44840684512882306\n",
    "* The AUROC of Fibrosis is 0.5362195673894978\n",
    "* The AUROC of Pleural_Thickening is 0.5302204565319748\n",
    "* The AUROC of Hernia is 0.43561190809913514\n",
    "Predict Time: 0:03:35.336959\n",
    "\n",
    "#### Model Trained 15 epochs, transform1\n",
    "\n",
    "* The average AUROC is 0.626\n",
    "* The AUROC of Atelectasis is 0.6099339225725543\n",
    "* The AUROC of Cardiomegaly is 0.6010881987356465\n",
    "* The AUROC of Effusion is 0.7138617014446589\n",
    "* The AUROC of Infiltration is 0.5488878033845505\n",
    "* The AUROC of Mass is 0.5329107421959458\n",
    "* The AUROC of Nodule is 0.5597421920386231\n",
    "* The AUROC of Pneumonia is 0.5141474040099794\n",
    "* The AUROC of Pneumothorax is 0.68892335847419\n",
    "* The AUROC of Consolidation is 0.6874902348764493\n",
    "* The AUROC of Edema is 0.789283092744088\n",
    "* The AUROC of Emphysema is 0.6213053225614307\n",
    "* The AUROC of Fibrosis is 0.6504512954572436\n",
    "* The AUROC of Pleural_Thickening is 0.5792289384426821\n",
    "* The AUROC of Hernia is 0.663563257739683\n",
    "\n",
    "### Model Trained with 15 epochs and scheduler, transform1\n",
    "* The average AUROC is 0.679\n",
    "* The AUROC of Atelectasis is 0.6406096981917049\n",
    "* The AUROC of Cardiomegaly is 0.6800817382395566\n",
    "* The AUROC of Effusion is 0.7288424773864505\n",
    "* The AUROC of Infiltration is 0.5384996004973043\n",
    "* The AUROC of Mass is 0.5753880650294986\n",
    "* The AUROC of Nodule is 0.6035420785256662\n",
    "* The AUROC of Pneumonia is 0.5957072608616284\n",
    "* The AUROC of Pneumothorax is 0.7298815534886969\n",
    "* The AUROC of Consolidation is 0.7119627809094822\n",
    "* The AUROC of Edema is 0.822888148544922\n",
    "* The AUROC of Emphysema is 0.7044018976469857\n",
    "* The AUROC of Fibrosis is 0.7071243037968423\n",
    "* The AUROC of Pleural_Thickening is 0.6461906776431365\n",
    "* The AUROC of Hernia is 0.8141235894819112\n",
    "\n",
    "\n",
    "### Model3 Trained with 5 epochs and scheduler, transform2, doing finetuning\n",
    "\n",
    "The average AUROC is 0.788\n",
    "* The AUROC of Atelectasis is 0.7489327514696104\n",
    "* The AUROC of Cardiomegaly is 0.8624223503416083\n",
    "* The AUROC of Effusion is 0.8481957794094263\n",
    "* The AUROC of Infiltration is 0.5208833490502071\n",
    "* The AUROC of Mass is 0.8112666095907779\n",
    "* The AUROC of Nodule is 0.7241843444330321\n",
    "* The AUROC of Pneumonia is 0.707121858646708\n",
    "* The AUROC of Pneumothorax is 0.8551896787260633\n",
    "* The AUROC of Consolidation is 0.7767152224992842\n",
    "* The AUROC of Edema is 0.8971054331868876\n",
    "*  The AUROC of Emphysema is 0.8611092544828925\n",
    "* The AUROC of Fibrosis is 0.7887263006509438\n",
    "* The AUROC of Pleural_Thickening is 0.7237187698617101\n",
    "* The AUROC of Hernia is 0.9127174313230288\n",
    "Predict Time: 0:03:04.180466\n",
    "\n",
    "### Model4 Trained over \"model3\" with 5 epochs and scheduler, transform2, doing finetuning\n",
    "\n",
    "The average AUROC is 0.790\n",
    "* The AUROC of Atelectasis is 0.7473135574453651\n",
    "* The AUROC of Cardiomegaly is 0.872459265342519\n",
    "* The AUROC of Effusion is 0.8441989498985264\n",
    "* The AUROC of Infiltration is 0.5442143078958999\n",
    "* The AUROC of Mass is 0.8140306853786841\n",
    "* The AUROC of Nodule is 0.7218175283825453\n",
    "* The AUROC of Pneumonia is 0.7056280044420105\n",
    "* The AUROC of Pneumothorax is 0.8604332962402138\n",
    "* The AUROC of Consolidation is 0.7728041755325066\n",
    "* The AUROC of Edema is 0.8994701265347935\n",
    "* The AUROC of Emphysema is 0.8670938030633245\n",
    "* The AUROC of Fibrosis is 0.8028644493688417\n",
    "* The AUROC of Pleural_Thickening is 0.7183141564743182\n",
    "* The AUROC of Hernia is 0.8934911089724129\n",
    "Predict Time: 0:03:00.684701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
